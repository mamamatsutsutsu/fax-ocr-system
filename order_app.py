import streamlit as st
import google.generativeai as genai
from PIL import Image, ImageEnhance, ImageFilter
import pandas as pd
import json
import re
import time
import io
from datetime import datetime
import fitz  # PyMuPDF
from reportlab.pdfgen import canvas
from reportlab.lib.pagesizes import A4
from reportlab.lib.units import mm

DEFAULT_MAX_PDF_PAGES = 5
DEFAULT_DPI = 220
DEFAULT_TEMPERATURE = 0.1
DEFAULT_MAX_OUTPUT_TOKENS = 1400  # „Éò„ÉÉ„ÉÄ„Éº+ÊòéÁ¥∞+ÂÇôËÄÉ„ÅåÂ¢ó„Åà„ÇÑ„Åô„ÅÑ„ÅÆ„ÅßÂ∞ë„Åó‰ΩôË£ï
_JSON_OBJ_RE = re.compile(r"\{[\s\S]*\}")

def safe_json_object_extract(text: str):
    if not text:
        return None
    try:
        s = text.find("{")
        e = text.rfind("}") + 1
        if s != -1 and e != -1 and e > s:
            return json.loads(text[s:e])
    except Exception:
        pass
    try:
        cleaned = text.replace("```json", "").replace("```", "").strip()
        s = cleaned.find("{")
        e = cleaned.rfind("}") + 1
        if s != -1 and e != -1 and e > s:
            return json.loads(cleaned[s:e])
    except Exception:
        pass
    try:
        m = _JSON_OBJ_RE.search(text)
        if m:
            return json.loads(m.group(0))
    except Exception:
        pass
    return None

def preprocess_image(img: Image.Image, contrast: float, sharpen: bool, binarize: bool) -> Image.Image:
    x = img.convert("L")
    x = ImageEnhance.Contrast(x).enhance(contrast)
    if sharpen:
        x = x.filter(ImageFilter.SHARPEN)
    if binarize:
        x = x.point(lambda p: 255 if p > 160 else 0)
    return x

def pdf_to_images(pdf_bytes: bytes, max_pages: int, dpi: int) -> tuple[list[Image.Image], int, int]:
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    total_pages = len(doc)
    use_pages = min(total_pages, max_pages)
    images = []
    for i in range(use_pages):
        page = doc.load_page(i)
        pix = page.get_pixmap(dpi=dpi)
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)
    return images, total_pages, use_pages

@st.cache_resource
def get_model(api_key: str, model_name: str, temperature: float, max_output_tokens: int):
    genai.configure(api_key=api_key)
    return genai.GenerativeModel(
        model_name,
        generation_config={"temperature": temperature, "max_output_tokens": max_output_tokens},
    )

def build_prompt() -> str:
    return """
„ÅÇ„Å™„Åü„ÅØ„ÄåÊúâÈôê‰ºöÁ§æ„Çø„Ç∑„É≠ÔºàÁ¶èÂ≤°Áúå‰πÖÁïôÁ±≥Â∏ÇÔºâ„Äç„ÅÆFAXÂèóÊ≥®„Çí„Éá„Éº„ÇøÂåñ„Åô„ÇãÂ∞ÇÁî®AI„Åß„Åô„ÄÇ
ÈáçË¶ÅÔºö„Çø„Ç∑„É≠ÂÅ¥ÔºàÂèóÊ≥®ÂÖà/ÈÄÅ„ÇäÂÖàÔºâ„ÅÆÂõ∫ÂÆöÊÉÖÂ†±„ÅØÊäΩÂá∫„Åó„Å™„ÅÑ„ÄÇÊäΩÂá∫ÂØæË±°„ÅØ‚ÄúÈ°ßÂÆ¢ÂÅ¥‚Äù„Å®‚ÄúÊ≥®ÊñáÊòéÁ¥∞‚Äù„Å®‚ÄúÈÖçÈÄÅ/ÁèæÂ†¥ÊåáÁ§∫‚Äù„ÅÆ„Åø„ÄÇ
ÂèóÊ≥®ÂÖàÔºàÂõ∫ÂÆö„ÉªÂèÇËÄÉÔºâÔºöÊúâÈôê‰ºöÁ§æ„Çø„Ç∑„É≠Ôºè„Äí839-0826 Á¶èÂ≤°Áúå‰πÖÁïôÁ±≥Â∏ÇÂ±±Êú¨Áî∫ËÄ≥Á¥ç295-1ÔºèTEL 0942-43-2138ÔºèFAX 0942-43-1950Ôºà„Åì„Çå„ÅØÊäΩÂá∫„Åó„Å™„ÅÑÔºâ

ÂøÖ„Åö„ÄêJSON„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„Äë„Å†„Åë„ÇíÂá∫Âäõ„Åô„Çã„Åì„Å®„ÄÇÂâçÁΩÆ„Åç/Ë™¨Êòé/„Ç≥„É°„É≥„Éà/„Ç≥„Éº„Éâ„Éñ„É≠„ÉÉ„ÇØ„ÅØÁ¶ÅÊ≠¢„ÄÇ
Âá∫Âäõ„ÅØÂøÖ„Åö { „ÅßÂßã„Åæ„Çä } „ÅßÁµÇ„Çè„Çã„Åì„Å®„ÄÇnull„ÅØ‰Ωø„Çè„Åö„ÄÅÊú™Áü•„ÅØ ""ÔºàÁ©∫ÊñáÂ≠óÔºâ„ÄÇ

FAX„ÅØÊâãÊõ∏„Åç„ÅåÂ§ö„Åè„ÄÅÈÖçÁΩÆÂõ≥„ÉªÊ§çÊ†ΩÂõ≥„ÉªÁü¢Âç∞„ÉªÁèæÂ†¥„Çπ„Ç±„ÉÉ„ÉÅ„ÉªÂú∞Âõ≥„ÅÆ„Çà„ÅÜ„Å™Áµµ„ÅåÊ∑∑Âú®„Åô„Çã„ÄÇ
Áµµ„ÇÑÈÖçÁΩÆÂõ≥„ÅØ„ÄåÈÖçÈÄÅ„ÉªÈÖçÁΩÆÊåáÁ§∫„Äç„Å™„ÅÆ„Åß„ÄÅË™≠„ÇÅ„ÇãÁØÑÂõ≤„Åß payment_or_notes „Å´Ë¶ÅÁ¥Ñ„Åó„Å¶ÂÖ•„Çå„ÇãÔºàÂïÜÂìÅË°å„Å®„Åó„Å¶Â¢ó„ÇÑ„Åï„Å™„ÅÑÔºâ„ÄÇ
„Åü„Å†„ÅóÈÖçÁΩÆÂõ≥„ÅÆ‰∏≠„Å´„ÄåÊ®πÁ®ÆÂêç + Êï∞Èáè/Ë¶èÊ†º„Äç„ÅåÊòéÁ¢∫„Å´Êõ∏„Åã„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÅØ items „Å´ÂèçÊò†„Åó„ÄÅÈÖçÁΩÆÊåáÁ§∫„ÅØÂÇôËÄÉ„Å∏ÊÆã„Åô„ÄÇ

ÂèñÊâ±„ÅÑ„ÅØÊ®πÊú®„ÉªÂ∫≠Êú®„ÉªÈâ¢Áâ©„ÉªÂú∞Ë¢´È°û„Å™„Å©Ôºà‰æãÔºö„ÇØ„É≠„É¢„Ç∏„ÄÅ„Ç≥„É´„Éá„Ç£„É™„ÉçÁ≠âÔºâ„ÄÇÊ®πÊú®ÁâπÊúâ„ÅÆË¶èÊ†ºË°®Ë®ò„ÇíÊÉ≥ÂÆö„Åó„ÄÅÊ¨°„ÇíÂÑ™ÂÖàÁöÑ„Å´Êãæ„ÅÜÔºö
HÔºàÊ®πÈ´òÔºâ/WÔºàËëâÂºµÔºâ/CÔºàÂππÂë®Ôºâ/ÁõÆÈÄö„Çä/Ê†πÈâ¢/Èú≤Âú∞/Èâ¢/ÂçòÊú®/Ê†™Á´ã/Áéâ/ÊîØÊü±/ÊûùÂºµ/Êê¨ÂÖ•Êù°‰ª∂ „Å™„Å© ‚Üí item_size_spec „Å´ÈõÜÁ¥Ñ„Åô„Çã„ÄÇ

„Äê„Éò„ÉÉ„ÉÄ„Éº„ÅÆËÄÉ„ÅàÊñπ„Äë
1ÊûöÔºà1È°ßÂÆ¢„ÅÆFAX/Ë§áÊï∞„Éö„Éº„Ç∏Ôºâ„Å´„Å§„Åç„ÄÅ„Éò„ÉÉ„ÉÄ„ÉºÔºàÈ°ßÂÆ¢Âêç/ÈÄ£Áµ°ÂÖà/‰ΩèÊâÄ/Á¥çÂìÅÂÖà/ÊîØÊâï/Â∏åÊúõÊó•/Ê≥®ÊÑè‰∫ãÈ†ÖÔºâ„ÅØÂü∫Êú¨1„Å§„ÄÇ
„Éö„Éº„Ç∏„ÅåË§áÊï∞„ÅÆÂ†¥Âêà„ÄÅ„Éò„ÉÉ„ÉÄ„Éº„ÅØÁµ±ÂêàÔºàÁ©∫Ê¨Ñ„ÅØ‰ªñ„Éö„Éº„Ç∏„ÅÆÂÄ§„ÅßË£úÂÆåÔºâ„Åó„Å¶„Çà„ÅÑ„ÄÇ

„ÄêÊäΩÂá∫„É´„Éº„É´„Äë
- customer_nameÔºöÈ°ßÂÆ¢ÂêçÔºàÈÄ†Âúí‰ºöÁ§æ/ÊñΩÂ∑•‰ºöÁ§æ/Ê•≠ËÄÖÂêç/ÊãÖÂΩìËÄÖÂêç„Åå„ÅÇ„Çå„Å∞‰ΩµË®òÔºâ
- customer_tel / customer_faxÔºöË¶ã„Å§„Åã„Çå„Å∞„ÄÇÊï∞Â≠ó/„Éè„Ç§„Éï„É≥Ê∑∑Âú®OK„ÄÇË¶ã„Å§„Åã„Çâ„Å™„Åë„Çå„Å∞ ""„ÄÇ
- customer_addressÔºöÈ°ßÂÆ¢‰ΩèÊâÄ„ÄÇshipping_addressÔºöÁ¥çÂìÅÂÖà/ÁèæÂ†¥/ÈÉµÈÄÅÂÖà„ÅåÂà•„Å™„Çâ„ÄÇ„Å™„Åë„Çå„Å∞ ""„ÄÇ
- payment_methodÔºöÊîØÊâïÊñπÊ≥ïÔºàÊéõ„Åë/ÁèæÈáë/‰ª£Âºï/ÊåØËæºÁ≠âÔºâ„ÅåÊòéÁ¢∫„Å™„Çâ„ÄÇÊõñÊòß„Å™„Çâ payment_or_notes „Å´„ÄÇ
- payment_or_notesÔºöÈÖçÈÄÅÂ∏åÊúõÊó•/ÊôÇÈñìÂ∏Ø/Áõ¥ÈÄÅ/ÁèæÂ†¥Âêç/ÁΩÆ„ÅçÂ†¥ÊåáÁ§∫/Êê¨ÂÖ•ÁµåË∑Ø/Á´ãÂÖ•Êù°‰ª∂/Ëá≥ÊÄ•/ÈÄ£Áµ°‰∫ãÈ†Ö/ÈÖçÁΩÆÂõ≥„ÅÆË¶ÅÁ¥Ñ„Å™„Å©„Çí„Åæ„Å®„ÇÅ„Å¶Ë®òËºâ„ÄÇ
- order_dateÔºöFAXË®òËºâ„ÅÆÊ≥®ÊñáÊó•„ÄÇ„Å™„Åë„Çå„Å∞ ""„ÄÇ
- itemsÔºöÊòéÁ¥∞„ÄÇ1Ë°å„Åî„Å®„Å´ item_nameÔºàÊ®πÁ®Æ/ÂìÅÁ®ÆÔºâ„ÄÅitem_size_specÔºàË¶èÊ†º/„Çµ„Ç§„Ç∫/Êù°‰ª∂Ôºâ„ÄÅquantityÔºàÂçäËßíÊï∞Â≠ó„ÅÆ„ÅøÔºâ„ÄÅunitÔºàÊú¨/Ê†™/Èâ¢/„Ç±„Éº„ÇπÁ≠âÔºâ„ÄÅunit_price„ÄÅline_total„ÄÇ
- grand_totalÔºöÂêàË®à„ÅåÊõ∏„Åã„Çå„Å¶„ÅÑ„Çå„Å∞„ÄÇ„Å™„Åë„Çå„Å∞ ""„ÄÇ

„ÄêÂá∫Âäõ„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÔºàÂõ∫ÂÆö„Ç≠„ÉºÔºâ„Äë
{
  "order_date": "YYYY/MM/DD",
  "customer_name": "",
  "customer_tel": "",
  "customer_fax": "",
  "customer_address": "",
  "shipping_address": "",
  "payment_method": "",
  "payment_or_notes": "",
  "items": [
    {
      "item_name": "",
      "item_size_spec": "",
      "quantity": "",
      "unit": "",
      "unit_price": "",
      "line_total": ""
    }
  ],
  "grand_total": ""
}
""".strip()

HEADER_KEYS = [
    "order_date",
    "customer_name",
    "customer_tel",
    "customer_fax",
    "customer_address",
    "shipping_address",
    "payment_method",
    "payment_or_notes",
    "grand_total",
]
ITEM_KEYS = ["item_name", "item_size_spec", "quantity", "unit", "unit_price", "line_total"]

def _clean_str(x) -> str:
    return str(x or "").strip()

def merge_order_objects(base: dict, incoming: dict) -> dict:
    if not isinstance(base, dict):
        base = {}
    if not isinstance(incoming, dict):
        return base
    for k in HEADER_KEYS:
        if not _clean_str(base.get(k)) and _clean_str(incoming.get(k)):
            base[k] = incoming.get(k)
    base_items = base.get("items", [])
    if not isinstance(base_items, list):
        base_items = []
    inc_items = incoming.get("items", [])
    if isinstance(inc_items, list):
        for it in inc_items:
            if isinstance(it, dict):
                base_items.append(it)
    base["items"] = base_items
    if not _clean_str(base.get("grand_total")) and _clean_str(incoming.get("grand_total")):
        base["grand_total"] = incoming.get("grand_total")
    return base

def normalize_order_object_to_rows(obj: dict, meta: dict) -> tuple[dict | None, pd.DataFrame]:
    if not isinstance(obj, dict):
        return None, pd.DataFrame()
    header = {k: _clean_str(obj.get(k)) for k in HEADER_KEYS}
    items = obj.get("items", [])
    if not isinstance(items, list):
        items = []
    rows = []
    for it in items:
        if not isinstance(it, dict):
            continue
        row = {k: _clean_str(it.get(k)) for k in ITEM_KEYS}
        row.update(header)
        row.update(meta)
        rows.append(row)
    if not rows:
        dummy = {k: "" for k in ITEM_KEYS}
        dummy.update(header)
        dummy.update(meta)
        rows.append(dummy)
    return header, pd.DataFrame(rows)

def create_simple_pdf(df: pd.DataFrame) -> io.BytesIO:
    buf = io.BytesIO()
    c = canvas.Canvas(buf, pagesize=A4)
    y = 285 * mm
    c.setFont("Helvetica", 14)
    c.drawString(20 * mm, y, "Order Summary (From Fax)")
    y -= 12 * mm
    c.setFont("Helvetica", 9)
    c.drawString(20 * mm, y, f"Issued: {datetime.now().strftime('%Y-%m-%d')}")
    y -= 10 * mm
    if len(df) > 0:
        r0 = df.iloc[0]
        c.drawString(20 * mm, y, f"Customer: {str(r0.get('customer_name',''))}")
        y -= 6 * mm
        c.drawString(20 * mm, y, f"TEL: {str(r0.get('customer_tel',''))}  FAX: {str(r0.get('customer_fax',''))}")
        y -= 6 * mm
        c.drawString(20 * mm, y, f"Address: {str(r0.get('customer_address',''))[:70]}")
        y -= 8 * mm
    headers = ["item_name", "item_size_spec", "quantity", "unit", "unit_price", "line_total"]
    colx = [15, 75, 130, 145, 160, 180]
    c.setFont("Helvetica", 8)
    for i, h in enumerate(headers):
        c.drawString(colx[i] * mm, y, h)
    y -= 4 * mm
    c.line(12 * mm, y, 198 * mm, y)
    y -= 6 * mm
    for _, row in df.iterrows():
        if y < 15 * mm:
            c.showPage()
            y = 285 * mm
            c.setFont("Helvetica", 8)
        vals = [
            str(row.get("item_name", ""))[:22],
            str(row.get("item_size_spec", ""))[:18],
            str(row.get("quantity", ""))[:6],
            str(row.get("unit", ""))[:6],
            str(row.get("unit_price", ""))[:10],
            str(row.get("line_total", ""))[:10],
        ]
        for i, v in enumerate(vals):
            c.drawString(colx[i] * mm, y, v)
        y -= 7 * mm
    c.save()
    buf.seek(0)
    return buf

def run():
    api_key = st.secrets.get("GOOGLE_API_KEY")
    if not api_key:
        st.error("‚ö† GOOGLE_API_KEY „ÅåË®≠ÂÆö„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇ")
        st.info('Streamlit Cloud ‚Üí Manage app ‚Üí Settings ‚Üí Secrets „Å´ `GOOGLE_API_KEY="..."` „ÇíË®≠ÂÆö„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ')
        st.stop()
    if "orders_rows" not in st.session_state:
        st.session_state.orders_rows = pd.DataFrame()
    if "orders_header" not in st.session_state:
        st.session_state.orders_header = []
    if "logs" not in st.session_state:
        st.session_state.logs = []
    st.title("üå≤ FAX Order Intelligence")
    st.caption("TASHIRO optimized: header unified + items extracted + sketch/diagram summarized")
    with st.expander("‚öôÔ∏è Processing Settings", expanded=True):
        col1, col2, col3 = st.columns(3)
        with col1:
            max_pages = st.slider("PDF page limit", 1, 30, DEFAULT_MAX_PDF_PAGES, 1)
            dpi = st.slider("PDF render DPI", 120, 320, DEFAULT_DPI, 10)
        with col2:
            temperature = st.slider("Temperature (stability)", 0.0, 1.0, DEFAULT_TEMPERATURE, 0.05)
            max_output_tokens = st.slider("Max output tokens (cost control)", 200, 3500, DEFAULT_MAX_OUTPUT_TOKENS, 100)
        with col3:
            contrast = st.slider("Preprocess contrast", 1.0, 3.0, 2.2, 0.1)
            sharpen = st.checkbox("Sharpen", True)
            binarize = st.checkbox("Binarize", False)
        retry_json = st.checkbox("Auto-retry if JSON parse fails (1 retry)", True)
        show_raw = st.checkbox("Debug: show raw model response", False)
        model_label = st.selectbox("AI Model", ["Gemini 2 Flash", "Gemini 2 Flash Lite"], index=0)
        model_name = "gemini-2.0-flash" if model_label == "Gemini 2 Flash" else "gemini-2.0-flash-lite"
    model = get_model(api_key, model_name, temperature, max_output_tokens)
    prompt = build_prompt()
    st.divider()
    uploaded_files = st.file_uploader(
        "Upload FAX files (JPG/PNG/JPEG/PDF)",
        type=["jpg", "png", "jpeg", "pdf"],
        accept_multiple_files=True,
    )
    analyze_clicked = st.button("üöÄ Start AI Analysis", type="primary", use_container_width=True)
    if analyze_clicked:
        if not uploaded_files:
            st.warning("„Éï„Ç°„Ç§„É´„Çí„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ")
            st.stop()
        total_tasks = 0
        plan = []
        for uf in uploaded_files:
            if uf.type == "application/pdf":
                b = uf.getvalue()
                try:
                    doc = fitz.open(stream=b, filetype="pdf")
                    total_pages = len(doc)
                    use_pages = min(total_pages, max_pages)
                    total_tasks += use_pages
                    plan.append({"type": "pdf", "name": uf.name, "bytes": b, "total_pages": total_pages, "use_pages": use_pages})
                except Exception as e:
                    st.session_state.logs.append({
                        "timestamp": datetime.now().isoformat(),
                        "status": "pdf_open_error",
                        "file": uf.name,
                        "detail": str(e),
                    })
            else:
                total_tasks += 1
                plan.append({"type": "img", "name": uf.name, "file": uf})
        if total_tasks == 0:
            st.error("Ëß£ÊûêÂØæË±°„Åå„ÅÇ„Çä„Åæ„Åõ„ÇìÔºàPDF„ÅåÂ£ä„Çå„Å¶„ÅÑ„Çã/„Éö„Éº„Ç∏Âà∂Èôê/ÂΩ¢Âºè„ÅÆÂïèÈ°åÔºâ„ÄÇ")
            st.stop()
        prog = st.progress(0)
        status = st.empty()
        all_rows = []
        all_headers = []
        completed = 0
        def call_model(img_for_model: Image.Image, reprompt: bool = False) -> str:
            p = prompt if not reprompt else (prompt + "\n\n„ÄêÂÜçÂá∫ÂäõÊåáÁ§∫„ÄëÂøÖ„ÅöJSON„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„ÅÆ„Åø„ÄÇË™¨Êòé/„Ç≥„Éº„Éâ„Éñ„É≠„ÉÉ„ÇØÁ¶ÅÊ≠¢„ÄÇ")
            resp = model.generate_content([p, img_for_model])
            return getattr(resp, "text", "") or ""
        for item in plan:
            if item["type"] == "pdf":
                images, total_pages, use_pages = pdf_to_images(item["bytes"], max_pages=item["use_pages"], dpi=dpi)
                if total_pages > max_pages:
                    st.info(f"‚Ñπ {item['name']}: {total_pages} pages detected ‚Üí analyzing first {use_pages} pages.")
                merged_obj = {}
                for page_idx, img in enumerate(images, start=1):
                    status.text(f"Processing: {item['name']} (page {page_idx}/{use_pages}) | {completed+1}/{total_tasks}")
                    t0 = time.time()
                    try:
                        pre = preprocess_image(img, contrast=contrast, sharpen=sharpen, binarize=binarize)
                        raw = call_model(pre, reprompt=False)
                        obj = safe_json_object_extract(raw)
                        if obj is None and retry_json:
                            raw2 = call_model(pre, reprompt=True)
                            obj = safe_json_object_extract(raw2)
                            if show_raw:
                                st.write(f"RAW (retry) {item['name']} p{page_idx}:")
                                st.code(raw2)
                        if show_raw:
                            st.write(f"RAW {item['name']} p{page_idx}:")
                            st.code(raw)
                        if isinstance(obj, dict):
                            merged_obj = merge_order_objects(merged_obj, obj)
                            status_flag = "success"
                        else:
                            status_flag = "parse_failed"
                        st.session_state.logs.append({
                            "timestamp": datetime.now().isoformat(),
                            "status": status_flag,
                            "file": item["name"],
                            "page": page_idx,
                            "model": model_name,
                            "elapsed_sec": round(time.time() - t0, 3),
                        })
                    except Exception as e:
                        st.session_state.logs.append({
                            "timestamp": datetime.now().isoformat(),
                            "status": "error",
                            "file": item["name"],
                            "page": page_idx,
                            "model": model_name,
                            "detail": str(e),
                        })
                    completed += 1
                    prog.progress(min(1.0, completed / total_tasks))
                    time.sleep(0.03)
                meta = {"ÂÖÉ„Éï„Ç°„Ç§„É´": item["name"], "„Éö„Éº„Ç∏": f"1-{use_pages}"}
                header, rows_df = normalize_order_object_to_rows(merged_obj, meta)
                if header:
                    all_headers.append({**header, **meta})
                if not rows_df.empty:
                    all_rows.append(rows_df)
            else:
                status.text(f"Processing: {item['name']} | {completed+1}/{total_tasks}")
                t0 = time.time()
                try:
                    img = Image.open(item["file"])
                    pre = preprocess_image(img, contrast=contrast, sharpen=sharpen, binarize=binarize)
                    raw = call_model(pre, reprompt=False)
                    obj = safe_json_object_extract(raw)
                    if obj is None and retry_json:
                        raw2 = call_model(pre, reprompt=True)
                        obj = safe_json_object_extract(raw2)
                        if show_raw:
                            st.write(f"RAW (retry) {item['name']}:")
                            st.code(raw2)
                    if show_raw:
                        st.write(f"RAW {item['name']}:")
                        st.code(raw)
                    meta = {"ÂÖÉ„Éï„Ç°„Ç§„É´": item["name"], "„Éö„Éº„Ç∏": ""}
                    header, rows_df = normalize_order_object_to_rows(obj if isinstance(obj, dict) else {}, meta)
                    if header:
                        all_headers.append({**header, **meta})
                    if not rows_df.empty:
                        all_rows.append(rows_df)
                    st.session_state.logs.append({
                        "timestamp": datetime.now().isoformat(),
                        "status": "success" if isinstance(obj, dict) else "parse_failed",
                        "file": item["name"],
                        "model": model_name,
                        "elapsed_sec": round(time.time() - t0, 3),
                    })
                except Exception as e:
                    st.session_state.logs.append({
                        "timestamp": datetime.now().isoformat(),
                        "status": "error",
                        "file": item["name"],
                        "model": model_name,
                        "detail": str(e),
                    })
                completed += 1
                prog.progress(min(1.0, completed / total_tasks))
                time.sleep(0.03)
        status.empty()
        prog.empty()
        st.session_state.orders_header = all_headers
        st.session_state.orders_rows = pd.concat(all_rows, ignore_index=True) if all_rows else pd.DataFrame()
        if not st.session_state.orders_rows.empty:
            st.success(f"Done. Extracted {len(st.session_state.orders_rows)} detail rows (header unified per fax).")
        else:
            st.warning("ÊòéÁ¥∞Ë°å„ÅåÊäΩÂá∫„Åß„Åç„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇLogs„ÇíÁ¢∫Ë™ç„Åó„ÄÅDPI/„Ç≥„É≥„Éà„É©„Çπ„Éà„Çí‰∏ä„Åí„Çã„Åã„ÄÅFAXÁîªÂÉè„ÇíÊòé„Çã„ÅèÊíÆÂΩ±„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ")
    if st.session_state.orders_header:
        st.divider()
        st.subheader("üßë‚Äçüíº Order Header (per Fax)")
        header_df = pd.DataFrame(st.session_state.orders_header)
        st.dataframe(header_df, use_container_width=True, hide_index=True)
        header_csv = header_df.to_csv(index=False).encode("utf-8-sig")
        st.download_button(
            "‚¨áÔ∏è Download Header CSV",
            data=header_csv,
            file_name=f"fax_headers_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
            mime="text/csv",
            use_container_width=True,
        )
    if not st.session_state.orders_rows.empty:
        st.divider()
        st.subheader("üßæ Order Line Items (editable)")
        edited = st.data_editor(
            st.session_state.orders_rows,
            use_container_width=True,
            num_rows="dynamic",
        )
        c1, c2, c3 = st.columns(3)
        with c1:
            csv = edited.to_csv(index=False).encode("utf-8-sig")
            st.download_button(
                "‚¨áÔ∏è Download Items CSV",
                data=csv,
                file_name=f"fax_items_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
                mime="text/csv",
                use_container_width=True,
            )
        with c2:
            if st.button("üßæ Generate PDF Summary", use_container_width=True):
                pdf_buf = create_simple_pdf(edited)
                st.download_button(
                    "‚¨áÔ∏è Download PDF",
                    data=pdf_buf,
                    file_name="order_summary.pdf",
                    mime="application/pdf",
                    use_container_width=True,
                )
        with c3:
            if st.button("üßπ Clear Results", use_container_width=True):
                st.session_state.orders_rows = pd.DataFrame()
                st.session_state.orders_header = []
                st.rerun()
    with st.expander("üìú Logs / Diagnostics", expanded=False):
        log_df = pd.DataFrame(st.session_state.logs)
        if log_df.empty:
            st.caption("No logs yet.")
        else:
            st.dataframe(log_df, use_container_width=True, hide_index=True)
            log_csv = log_df.to_csv(index=False).encode("utf-8")
            st.download_button(
                "‚¨áÔ∏è Download Logs CSV",
                data=log_csv,
                file_name=f"analysis_logs_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
                mime="text/csv",
                use_container_width=True,
            )
            if st.button("Clear Logs"):
                st.session_state.logs = []
                st.rerun()